{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0wzcWULKengu"
      },
      "outputs": [],
      "source": [
        "#! pip install ../input/talibsourceinstall/talib_binary-0.4.19-cp37-cp37m-manylinux1_x86_64.whl\n",
        "! pip install numpy --upgrade --ignore-installed\n",
        "! pip install -U tensorflow\n",
        "! pip install pandas-datareader\n",
        "! conda install TA-Lib -y\n",
        "! conda install pandas-ta -y\n",
        "#! pip install -U git+https://github.com/twopirllc/pandas-ta.git@development\n",
        "! pip install yfinance\n",
        "! pip install shap\n",
        "! pip install boruta\n",
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "! pip install keras\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "! pip install keras-tuner\n",
        "\n",
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "condacolab.install()\n",
        "! conda install TA-Lib -y\n",
        "! conda install pandas-ta -y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HQhew1iDe8y9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math as math\n",
        "import scipy as scip\n",
        "from scipy.stats import norm\n",
        "import tabulate as table\n",
        "import shap\n",
        "import boruta\n",
        "#tensorflow\n",
        "import tensorflow\n",
        "\n",
        "# Ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import datetime\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MI00v_Ihe9nw"
      },
      "outputs": [],
      "source": [
        "# check the tensorflow and keras version\n",
        "print(f\" TensorFlow version is {tensorflow.__version__} \\n Keras version is {tensorflow.keras.__version__}\")\n",
        "\n",
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard\n",
        "\n",
        "# tensorflow modules\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten, LSTM\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop \n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eAbqCkAve_us"
      },
      "outputs": [],
      "source": [
        "def set_seeds(seed=2022): \n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    tensorflow.random.set_seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i09t9zQ6jzHY"
      },
      "outputs": [],
      "source": [
        "def writeCSV(df, filename) :\n",
        "  filepath = r'/content/drive/MyDrive/Colab Notebooks/output data/'+filename+\".csv\"\n",
        "  df.to_csv(filepath)\n",
        "  print(\"Written CSV : \" + filepath)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SahaoLt-fCFc",
        "outputId": "e611114f-b895-4dd4-ce4f-5ea16b9eeffe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "131it [00:13,  9.83it/s]\n",
            "131it [00:08, 16.31it/s]\n",
            "131it [00:07, 16.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Written CSV : /content/drive/MyDrive/Colab Notebooks/output data/df_SPY.csv\n",
            "Written CSV : /content/drive/MyDrive/Colab Notebooks/output data/df_DIA.csv\n",
            "Written CSV : /content/drive/MyDrive/Colab Notebooks/output data/df_QQQ.csv\n",
            "(5684, 292)\n",
            "(5684, 292)\n"
          ]
        }
      ],
      "source": [
        "# ---------------------------------------------------------------------------------------------------------DATA SOURCING STEP------------------------------------------------------------------------\n",
        "#strategy\n",
        "import talib as tlb\n",
        "import pandas_ta as ta\n",
        "import yfinance as yf\n",
        "\n",
        "# download price information of index etfs and plug all the gaps in the data sets\n",
        "df_SPY = yf.download('SPY', start='2000-01-01', end='2022-08-05', progress=False )[['Open','High','Low', 'Close', 'Adj Close', 'Volume']]\n",
        "df_DIA = yf.download('DIA', start='2000-01-01', end='2022-08-05', progress=False )[['Open','High','Low', 'Close', 'Adj Close', 'Volume']]\n",
        "df_QQQ = yf.download('QQQ', start='2000-01-01', end='2022-08-05', progress=False )[['Open','High','Low', 'Close', 'Adj Close', 'Volume']]\n",
        "\n",
        "df_SPY.ta.strategy(\"all\") # backfill as most starting data points in new features are empty\n",
        "df_DIA.ta.strategy(\"all\")\n",
        "df_QQQ.ta.strategy(\"all\")\n",
        "\n",
        "\n",
        "\n",
        "#download economic indicators and plug all the gaps in the data sets\n",
        "df_GDP = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/input data/GDP.csv\").fillna(\"ffil\")\n",
        "df_CPI = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/input data/Monthly CPI.csv\").fillna(\"ffil\")\n",
        "df_UNEMP = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/input data//UNRATE.csv\").fillna(\"ffil\")\n",
        "df_30MRTG = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/input data//MORTGAGE30US.csv\").fillna(\"ffil\")\n",
        "df_VIX = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/input data//VIXCLS.csv\").fillna(\"ffil\")\n",
        "df_T3Bills = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/input data//TB3MS.csv\").fillna(\"ffil\")\n",
        "df_10YIR = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/input data//REAINTRATREARAT10Y.csv\").fillna(\"ffil\")\n",
        "\n",
        "\n",
        "#df_SPY.to_csv(\"/content/drive/MyDrive/Colab Notebooks/output data/df_SPY.csv\")\n",
        "writeCSV(df_SPY, \"df_SPY\")\n",
        "writeCSV(df_DIA, \"df_DIA\")\n",
        "writeCSV(df_QQQ, \"df_QQQ\")\n",
        "#df_DIA.to_csv(\"/content/drive/MyDrive/Colab Notebooks/output data/df_DIA.csv\")\n",
        "#df_QQQ.to_csv(\"/content/drive/MyDrive/Colab Notebooks/output data/df_QQQ.csv\")\n",
        "\n",
        "df_SPY = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/output data/df_SPY.csv\")\n",
        "df_DIA = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/output data/df_DIA.csv\")\n",
        "df_QQQ = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/output data/df_QQQ.csv\")\n",
        "\n",
        "# add ecomomic data points as additional features\n",
        "\n",
        "df_SPY['Date'] = pd.to_datetime(df_SPY['Date'])\n",
        "df_DIA['Date'] = pd.to_datetime(df_DIA['Date'])\n",
        "df_QQQ['Date'] = pd.to_datetime(df_QQQ['Date'])\n",
        "\n",
        "eco_data= [df_GDP, df_CPI, df_UNEMP, df_30MRTG, df_VIX, df_T3Bills, df_10YIR ]\n",
        "\n",
        "df_SPY_eco = df_SPY\n",
        "df_DIA_eco = df_DIA\n",
        "df_QQQ_eco = df_QQQ\n",
        "\n",
        "for ecodf in eco_data :    \n",
        "    ecodf['Date'] = pd.to_datetime(ecodf['Date'])\n",
        "    df_SPY_eco = pd.merge(df_SPY_eco, ecodf, right_on='Date', left_on='Date', how='left')    \n",
        "    df_DIA_eco = pd.merge(df_DIA_eco, ecodf, right_on='Date', left_on='Date', how='left')\n",
        "    df_QQQ_eco = pd.merge(df_QQQ_eco, ecodf, right_on='Date', left_on='Date', how='left')\n",
        "\n",
        "\n",
        "print(df_DIA_eco.shape)\n",
        "print(df_QQQ_eco.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LX_kQnY0fKbW",
        "outputId": "1c48aaec-cdf9-4e6a-e981-646f080aaf7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Written CSV : /content/drive/MyDrive/Colab Notebooks/output data/df_SPY_eco.csv\n"
          ]
        }
      ],
      "source": [
        "# # ---------------------------------------------------------------------------------------------------------DATA PRE PROCESSING STEP------------------------------------------------------------------------\n",
        "    \n",
        "df_SPY_eco = df_SPY_eco.fillna(method=\"ffill\")\n",
        "df_DIA_eco = df_DIA_eco.fillna(method=\"ffill\")\n",
        "df_QQQ_eco = df_QQQ_eco.fillna(method=\"ffill\")\n",
        "\n",
        "# for missing starting values\n",
        "df_SPY_eco=df_SPY_eco.fillna(method=\"bfill\")\n",
        "df_DIA_eco = df_DIA_eco.fillna(method=\"bfill\")\n",
        "df_QQQ_eco = df_QQQ_eco.fillna(method=\"bfill\")\n",
        "\n",
        "# for test purposes. All column are completly filled now\n",
        "writeCSV(df_SPY_eco, \"df_SPY_eco\")\n",
        "#df_SPY_eco.to_csv(\"/content/drive/MyDrive/Colab Notebooks/output data/df_SPY_eco.csv\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5YRmXK9fPJ6"
      },
      "source": [
        "In \"Python Machine Learning\" by Raschka the author provides some guidance on page 111 when to normalize (min-max scale) and when to standardize data:\n",
        "\n",
        "\"Although normalization via min-max scaling is a commonly used technique that is useful when we need values in a bounded interval, standardization can be more practical for many machine learning algorithms. The reason is that many linear models, such as the logistic regression and SVM, [...] initialize the weights to 0 or small random values close to 0. Using standardization, we center the feature columns at mean 0 with standard deviation 1 so that the feature columns take the form of a normal distribution, which makes it easier to learn the weights. Furthermore, standardization maintains useful information about outliers and makes the algorithm less sensitive to them in contrast to min-max scaling, which scales the data to a limited range of values.\"\n",
        "\n",
        "Further, Section 6.3 \"Preprocessing data\" of Scikit-learn documentation states:\n",
        "\n",
        "\"Standardization of datasets is a common requirement for many machine learning estimators implemented in scikit-learn; they might behave badly if the individual features do not more or less look like standard normally distributed data: Gaussian with zero mean and unit variance.\n",
        "\n",
        "In practice we often ignore the shape of the distribution and just transform the data to center it by removing the mean value of each feature, then scale it by dividing non-constant features by their standard deviation.\n",
        "\n",
        "For instance, many elements used in the objective function of a learning algorithm (such as the RBF kernel of Support Vector Machines or the l1 and l2 regularizers of linear models) assume that all features are centered around zero and have variance in the same order. If a feature has a variance that is orders of magnitude larger than others, it might dominate the objective function and make the estimator unable to learn from other features correctly as expected.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k80JfkrlfS6u"
      },
      "outputs": [],
      "source": [
        "# ---------------------------------------------------------------------------------------------------------DATA SCALING STEP------------------------------------------------------------------------\n",
        "from sklearn.linear_model import LinearRegression, Lasso\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "# scaling all the features using standard scalar to ensure standard normal N(0,1) distribution\n",
        "# Tranformation or pre-processing\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# we have to exlcude date columns in order to carry out transformation and feature selection. If not, then date will be treated as a feature\n",
        "fe_df_SPY = df_SPY_eco.loc[:, df_SPY_eco.columns!='Date']\n",
        "fe_df_DIA = df_DIA_eco.loc[:, df_DIA_eco.columns!='Date']\n",
        "fe_df_QQQ = df_QQQ_eco.loc[:, df_QQQ_eco.columns!='Date']\n",
        "\n",
        "# features \n",
        "SPY_feature_X = fe_df_SPY.to_numpy()\n",
        "DIA_feature_X = fe_df_DIA.to_numpy()\n",
        "QQQ_feature_X = fe_df_QQQ.to_numpy()\n",
        "\n",
        "print(SPY_feature_X)\n",
        "\n",
        "gaussian_scaler = StandardScaler()\n",
        "\n",
        "SPY_feature_Xt = gaussian_scaler.fit_transform(SPY_feature_X) # you have to fit data in before transforming\n",
        "DIA_feature_Xt = gaussian_scaler.fit_transform(DIA_feature_X) # you have to fit data in before transforming\n",
        "QQQ_feature_Xt = gaussian_scaler.fit_transform(QQQ_feature_X) # you have to fit data in before transforming\n",
        "\n",
        "\n",
        "stdTransform_df_SPY = pd.DataFrame(SPY_feature_Xt, columns=fe_df_SPY.keys())\n",
        "stdTransform_df_DIA = pd.DataFrame(DIA_feature_Xt, columns=fe_df_DIA.keys())\n",
        "stdTransform_df_QQQ = pd.DataFrame(QQQ_feature_Xt, columns=fe_df_QQQ.keys())\n",
        "\n",
        "print(stdTransform_df_SPY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2X6qLytEfdVu"
      },
      "source": [
        "Variance Inflation Factor : \n",
        "VIF starts at 1 and has no upper limit.\n",
        "VIF = 1, no correlation between the independent variable and the other variables\n",
        "VIF exceeding 5 or 10 indicates high multicollinearity between this independent variable and the others\n",
        "Multicollinearity can be detected using various methods and one such method is Variable Inflation Factors (VIF).\n",
        "VIF determines the strength of the correlation between the independent variables. \n",
        "It is predicted by taking a variable and regressing it against every other variable.\n",
        "In order to include as many features as possible for feature selection, we will consider all the features with VIF score < 10. The study of feature selection using VIF indicates that most features selected are stock technical indicators and a few economic indicators (like CPI). Please refer to attached excel spreadsheet for detail. For the most part, SPY, DIA and QQQ have similar features with ViF score under 10 with few variations in that DIA has more features than SPY with ViF score of less than 10. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "meHfIrVBfe-w"
      },
      "outputs": [],
      "source": [
        "#--start collecting shortlisted deatures\n",
        "feature_collection_list_SPY = []\n",
        "feature_collection_list_DIA = []\n",
        "feature_collection_list_QQQ = []\n",
        "\n",
        "def feature_collection(shortlistedFeatures, assetName) : \n",
        "    collection_list = pd.DataFrame(shortlistedFeatures, columns=[assetName])\n",
        "    pd.concat([feature_collection_list, collection_list])    \n",
        "    print(\"For asset : \", assetName, \"...Features count = \", feature_collection_list.shape , \"....Feature list after feature engineering: \" , feature_collection_list)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------------------------------------------------------START OF FEATURE ENGINEERING------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "8uqOebygtUQo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gyccUJ_Nfh2O"
      },
      "outputs": [],
      "source": [
        "# hence standard Gaussian scalar will be applied given that most features will align to standard distribution pattern\n",
        "\n",
        "\n",
        "#------------------------ start of Feature Engineering -----------------------------------------------#\n",
        "#----- We have about 293 features in all including economic indicators --------#\n",
        "\n",
        "\n",
        "# Variable Inflation Factor\n",
        "#VIF starts at 1 and has no upper limit\n",
        "#VIF = 1, no correlation between the independent variable and the other variables\n",
        "#VIF exceeding 5 or 10 indicates high multicollinearity between this independent variable and the others\n",
        "# Multicollinearity can be detected using various methods and one such method is Variable Inflation Factors (VIF).\n",
        "# VIF determines the strength of the correlation between the independent variables. \n",
        "# It is predicted by taking a variable and regressing it against every other variable.\n",
        "\n",
        "# Import VIF\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "# Recall we have transformed stock data during PRE-PROCESSING STEP and stored it Xt\n",
        "SPY_vif = pd.DataFrame()\n",
        "DIA_vif = pd.DataFrame()\n",
        "QQQ_vif = pd.DataFrame()\n",
        "\n",
        "SPY_vif[\"Features\"] = fe_df_SPY.columns # capture feature names\n",
        "DIA_vif[\"Features\"] = fe_df_DIA.columns # capture feature names\n",
        "QQQ_vif[\"Features\"] = fe_df_QQQ.columns # capture feature names\n",
        "\n",
        "SPY_vif[\"VIF Factor\"] = [variance_inflation_factor(SPY_feature_Xt, i) for i in range(SPY_feature_Xt.shape[1])] # We are computing ViF for every of features in tranformed stockdata set Xt\n",
        "DIA_vif[\"VIF Factor\"] = [variance_inflation_factor(DIA_feature_Xt, i) for i in range(DIA_feature_Xt.shape[1])] # We are computing ViF for every of features in tranformed stockdata set Xt\n",
        "QQQ_vif[\"VIF Factor\"] = [variance_inflation_factor(QQQ_feature_Xt, i) for i in range(QQQ_feature_Xt.shape[1])] # We are computing ViF for every of features in tranformed stockdata set Xt\n",
        "\n",
        "# for the considerations of DL we will choose features with VIF score of less than 10\n",
        "print(\"SPY Features with VIF Factor of less than 10 : \\n\" , SPY_vif[SPY_vif['VIF Factor']<10])\n",
        "print(\"DIA Features with VIF Factor of less than 10 : \\n\" , DIA_vif[DIA_vif['VIF Factor']<10])\n",
        "print(\"QQQ Features with VIF Factor of less than 10 : \\n\" , QQQ_vif[QQQ_vif['VIF Factor']<10])\n",
        "\n",
        "writeCSV(SPY_vif[SPY_vif['VIF Factor']<10], \"SPY_VIF_Factor_LessThanTen\")\n",
        "writeCSV(QQQ_vif[QQQ_vif['VIF Factor']<10], \"QQQ_VIF_Factor_LessThanTen\")\n",
        "writeCSV(DIA_vif[DIA_vif['VIF Factor']<10], \"DIA_VIF_Factor_LessThanTen\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfI4lzH6foWT",
        "outputId": "943123c0-387f-4db5-94df-3eaae4227177"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['ADOSC_3_10', 'ADX_14', 'AR_26', 'BR_26', 'CDL_2CROWS', 'CDL_3BLACKCROWS', 'CDL_3INSIDE', 'CDL_3LINESTRIKE', 'CDL_3OUTSIDE', 'CDL_3WHITESOLDIERS', 'CDL_ADVANCEBLOCK', 'CDL_BELTHOLD', 'CDL_CLOSINGMARUBOZU', 'CDL_COUNTERATTACK', 'CDL_DARKCLOUDCOVER', 'CDL_DOJISTAR', 'CDL_ENGULFING', 'CDL_EVENINGDOJISTAR', 'CDL_EVENINGSTAR', 'CDL_GAPSIDESIDEWHITE', 'CDL_GRAVESTONEDOJI', 'CDL_HAMMER', 'CDL_HANGINGMAN', 'CDL_HARAMI', 'CDL_HARAMICROSS', 'CDL_HIGHWAVE', 'CDL_HIKKAKE', 'CDL_HIKKAKEMOD', 'CDL_HOMINGPIGEON', 'CDL_IDENTICAL3CROWS', 'CDL_INNECK', 'CDL_INSIDE', 'CDL_INVERTEDHAMMER', 'CDL_LADDERBOTTOM', 'CDL_LONGLINE', 'CDL_MARUBOZU', 'CDL_MATCHINGLOW', 'CDL_MORNINGDOJISTAR', 'CDL_MORNINGSTAR', 'CDL_ONNECK', 'CDL_PIERCING', 'CDL_RICKSHAWMAN', 'CDL_SEPARATINGLINES', 'CDL_SHOOTINGSTAR', 'CDL_SHORTLINE', 'CDL_SPINNINGTOP', 'CDL_STALLEDPATTERN', 'CDL_STICKSANDWICH', 'CDL_TASUKIGAP', 'CDL_THRUSTING', 'CDL_TRISTAR', 'CDL_UNIQUE3RIVER', 'CDL_UPSIDEGAP2CROWS', 'CDL_XSIDEGAP3METHODS', 'CHOP_14_1_100', 'DPO_20', 'EOM_14_100000000', 'ER_10', 'KURT_30', 'KVOs_34_55_13', 'MASSI_9_25', 'MFI_14', 'PSARaf_0.02_0.2', 'PSARr_0.02_0.2', 'PVR', 'QQEl_14_5_4.236', 'QQEs_14_5_4.236', 'QS_10', 'SKEW_30', 'SQZPRO_ON_NARROW', 'STC_10_12_26_0.5', 'STCstoch_10_12_26_0.5', 'SUPERTd_7_3.0', 'THERMO_20_2_0.5', 'THERMOl_20_2_0.5', 'THERMOs_20_2_0.5', 'TTM_TRND_6', 'VHF_28', 'MEDCPIM158SFRBCLE']]\n",
            "[['ADOSC_3_10', 'ADX_14', 'AR_26', 'BR_26', 'CDL_2CROWS', 'CDL_3INSIDE', 'CDL_3LINESTRIKE', 'CDL_3OUTSIDE', 'CDL_3WHITESOLDIERS', 'CDL_ABANDONEDBABY', 'CDL_ADVANCEBLOCK', 'CDL_BELTHOLD', 'CDL_BREAKAWAY', 'CDL_CLOSINGMARUBOZU', 'CDL_COUNTERATTACK', 'CDL_DARKCLOUDCOVER', 'CDL_DOJISTAR', 'CDL_ENGULFING', 'CDL_EVENINGDOJISTAR', 'CDL_EVENINGSTAR', 'CDL_GAPSIDESIDEWHITE', 'CDL_GRAVESTONEDOJI', 'CDL_HAMMER', 'CDL_HANGINGMAN', 'CDL_HARAMI', 'CDL_HARAMICROSS', 'CDL_HIGHWAVE', 'CDL_HIKKAKE', 'CDL_HIKKAKEMOD', 'CDL_HOMINGPIGEON', 'CDL_IDENTICAL3CROWS', 'CDL_INNECK', 'CDL_INSIDE', 'CDL_INVERTEDHAMMER', 'CDL_LADDERBOTTOM', 'CDL_LONGLINE', 'CDL_MARUBOZU', 'CDL_MATCHINGLOW', 'CDL_MORNINGDOJISTAR', 'CDL_MORNINGSTAR', 'CDL_ONNECK', 'CDL_PIERCING', 'CDL_RICKSHAWMAN', 'CDL_RISEFALL3METHODS', 'CDL_SEPARATINGLINES', 'CDL_SHOOTINGSTAR', 'CDL_SHORTLINE', 'CDL_SPINNINGTOP', 'CDL_STALLEDPATTERN', 'CDL_STICKSANDWICH', 'CDL_TASUKIGAP', 'CDL_THRUSTING', 'CDL_TRISTAR', 'CDL_UNIQUE3RIVER', 'CDL_UPSIDEGAP2CROWS', 'CDL_XSIDEGAP3METHODS', 'CHOP_14_1_100', 'CMF_20', 'DPO_20', 'ER_10', 'KURT_30', 'MASSI_9_25', 'MFI_14', 'PSARaf_0.02_0.2', 'PSARr_0.02_0.2', 'PVR', 'QQEl_14_5_4.236', 'QQEs_14_5_4.236', 'QS_10', 'SKEW_30', 'SQZPRO_ON_NARROW', 'STC_10_12_26_0.5', 'STCstoch_10_12_26_0.5', 'SUPERTd_7_3.0', 'THERMO_20_2_0.5', 'THERMOl_20_2_0.5', 'THERMOs_20_2_0.5', 'TTM_TRND_6', 'VHF_28', 'MEDCPIM158SFRBCLE']]\n",
            "[['ADOSC_3_10', 'ADX_14', 'BBB_5_2.0', 'AR_26', 'BR_26', 'CDL_2CROWS', 'CDL_3BLACKCROWS', 'CDL_3INSIDE', 'CDL_3LINESTRIKE', 'CDL_3OUTSIDE', 'CDL_3WHITESOLDIERS', 'CDL_ADVANCEBLOCK', 'CDL_BELTHOLD', 'CDL_BREAKAWAY', 'CDL_CLOSINGMARUBOZU', 'CDL_COUNTERATTACK', 'CDL_DARKCLOUDCOVER', 'CDL_DOJISTAR', 'CDL_ENGULFING', 'CDL_EVENINGDOJISTAR', 'CDL_EVENINGSTAR', 'CDL_GAPSIDESIDEWHITE', 'CDL_GRAVESTONEDOJI', 'CDL_HAMMER', 'CDL_HANGINGMAN', 'CDL_HARAMI', 'CDL_HARAMICROSS', 'CDL_HIGHWAVE', 'CDL_HIKKAKE', 'CDL_HOMINGPIGEON', 'CDL_IDENTICAL3CROWS', 'CDL_INNECK', 'CDL_INSIDE', 'CDL_INVERTEDHAMMER', 'CDL_LADDERBOTTOM', 'CDL_LONGLINE', 'CDL_MARUBOZU', 'CDL_MATCHINGLOW', 'CDL_MATHOLD', 'CDL_MORNINGDOJISTAR', 'CDL_MORNINGSTAR', 'CDL_ONNECK', 'CDL_PIERCING', 'CDL_RICKSHAWMAN', 'CDL_RISEFALL3METHODS', 'CDL_SEPARATINGLINES', 'CDL_SHOOTINGSTAR', 'CDL_SHORTLINE', 'CDL_SPINNINGTOP', 'CDL_STALLEDPATTERN', 'CDL_STICKSANDWICH', 'CDL_TASUKIGAP', 'CDL_THRUSTING', 'CDL_TRISTAR', 'CDL_UNIQUE3RIVER', 'CDL_UPSIDEGAP2CROWS', 'CDL_XSIDEGAP3METHODS', 'CHOP_14_1_100', 'DPO_20', 'ER_10', 'KURT_30', 'MASSI_9_25', 'PSARaf_0.02_0.2', 'PSARr_0.02_0.2', 'PVR', 'QQEs_14_5_4.236', 'QS_10', 'SKEW_30', 'SQZPRO_ON_NARROW', 'STC_10_12_26_0.5', 'SUPERTd_7_3.0', 'THERMO_20_2_0.5', 'THERMOl_20_2_0.5', 'THERMOs_20_2_0.5', 'TTM_TRND_6', 'VHF_28', 'MEDCPIM158SFRBCLE']]\n"
          ]
        }
      ],
      "source": [
        "feature_collection_list_SPY.append(SPY_vif[SPY_vif['VIF Factor']<10]['Features'].values.tolist())\n",
        "feature_collection_list_DIA.append(DIA_vif[DIA_vif['VIF Factor']<10]['Features'].values.tolist())\n",
        "feature_collection_list_QQQ.append(QQQ_vif[QQQ_vif['VIF Factor']<10]['Features'].values.tolist())\n",
        "\n",
        "print(feature_collection_list_SPY)\n",
        "print(feature_collection_list_DIA)\n",
        "print(feature_collection_list_QQQ)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UqQAF8USfo9n",
        "outputId": "190edf6c-cc4d-4c77-d3aa-bbb518261bcd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Features Array Shape, SPY_X :  (5684, 291)\n",
            "Features Array Shape, DIA_X :  (5684, 291)\n",
            "Features Array Shape, QQQ_X :  (5684, 291)\n",
            "SPY Train size :  (4547, 291)  | SPY Test size :  (1137, 291)\n",
            "DIA Train size :  (4547, 291)  | DIA Test size :  (1137, 291)\n",
            "QQQ Train size :  (4547, 291)  | QQQ Test size :  (1137, 291)\n",
            "SPY Train size :  (4547,)  | SPY Test size :  (1137,)\n",
            "DIA Train size :  (4547,)  | DIA Test size :  (1137,)\n",
            "QQQ Train size :  (4547,)  | QQQ Test size :  (1137,)\n"
          ]
        }
      ],
      "source": [
        "# features \n",
        "# LOGRET_1\n",
        "\n",
        "# = SPY.to_numpy()\n",
        "print(\"Features Array Shape, SPY_X : \", SPY_feature_X.shape)\n",
        "print(\"Features Array Shape, DIA_X : \", DIA_feature_X.shape)\n",
        "print(\"Features Array Shape, QQQ_X : \", QQQ_feature_X.shape)\n",
        "# Target\n",
        "#SPY_feature_Yt = np.where(fe_df_SPY['LOGRET_1'] > 0, 1, 0 )\n",
        "#DIA_feature_Yt = np.where(fe_df_DIA['LOGRET_1'] > 0, 1, 0 )\n",
        "#QQQ_feature_Yt = np.where(fe_df_QQQ['LOGRET_1'] > 0, 1, 0 )\n",
        "\n",
        "SPY_feature_Yt = stdTransform_df_SPY['Adj Close'].to_numpy()\n",
        "DIA_feature_Yt = stdTransform_df_DIA['Adj Close'].to_numpy()\n",
        "QQQ_feature_Yt = stdTransform_df_QQQ['Adj Close'].to_numpy()\n",
        "\n",
        "# Splitting the datasets into training and testing data. Using 20pct of data for test sample\n",
        "# Always keep shuffle = False for financial time series\n",
        "SPY_feature_X_train, SPY_feature_X_test, SPY_feature_Yt_train, SPY_feature_Yt_test = train_test_split(SPY_feature_Xt, SPY_feature_Yt, test_size=0.2, random_state=0, shuffle=False)\n",
        "DIA_feature_X_train, DIA_feature_X_test, DIA_feature_Yt_train, DIA_feature_Yt_test = train_test_split(DIA_feature_Xt, DIA_feature_Yt, test_size=0.2, random_state=0, shuffle=False)\n",
        "QQQ_feature_X_train, QQQ_feature_X_test, QQQ_feature_Yt_train, QQQ_feature_Yt_test = train_test_split(QQQ_feature_Xt, QQQ_feature_Yt, test_size=0.2, random_state=0, shuffle=False)\n",
        "\n",
        "# Output the train and test data size\n",
        "print(\"SPY Train size : \", SPY_feature_X_train.shape, \" | SPY Test size : \", SPY_feature_X_test.shape)\n",
        "print(\"DIA Train size : \", DIA_feature_X_train.shape, \" | DIA Test size : \", DIA_feature_X_test.shape)\n",
        "print(\"QQQ Train size : \", QQQ_feature_X_train.shape, \" | QQQ Test size : \", QQQ_feature_X_test.shape)\n",
        "\n",
        "# Output the train and test data size\n",
        "print(\"SPY Train size : \", SPY_feature_Yt_train.shape, \" | SPY Test size : \", SPY_feature_Yt_test.shape)\n",
        "print(\"DIA Train size : \", DIA_feature_Yt_train.shape, \" | DIA Test size : \", DIA_feature_Yt_test.shape)\n",
        "print(\"QQQ Train size : \", QQQ_feature_Yt_train.shape, \" | QQQ Test size : \", QQQ_feature_Yt_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mwkuAuLlfs1n"
      },
      "outputs": [],
      "source": [
        "# ---------------------------------------------------------------------------------------------------------SELECT K BEST------------------------------------------------------------------------\n",
        "# select K best\n",
        "def runSelectKFeatures (f_regression, Xt, Yt, columnValues, assetName) : # Yt is the target. Xt is the transformed stock / asset price data\n",
        "    \n",
        "    scoring_method = SelectKBest(f_regression, k=100) # selecting top 100 features\n",
        "    scoring_method.fit(Xt, Yt)\n",
        "    i = 0\n",
        "    topRankingFeaturesList = []\n",
        "    \n",
        "    # Show selected features\n",
        "    topScoringFeatures = scoring_method.get_support(indices=True)    \n",
        "    df_selectKbestScores = pd.DataFrame(columns=['SelectKBest Score', 'feature_names'])\n",
        "    df_selectKbestScores['SelectKBest Score'] = scoring_method.scores_\n",
        "    df_selectKbestScores['feature_names'] = columnValues\n",
        "       \n",
        "    print(\"Top scoring features sequeunce number for Data : \", assetName, \" : \")\n",
        "    for items in topScoringFeatures :\n",
        "        print(df_selectKbestScores.iloc[items]['feature_names'])\n",
        "        topRankingFeaturesList.append(df_selectKbestScores.iloc[items]['feature_names'])\n",
        "        i = i + 1\n",
        "    return topRankingFeaturesList"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "haOfr92Vfu7Z"
      },
      "outputs": [],
      "source": [
        "# ---------------------------------------------------------------------------------------------------------RFE FEATURE ENGINEERING-----------------------------------------------------------------------\n",
        "# Feature Selection using RFE - Ridge, Lasso, LinearRegression\n",
        "\n",
        "def runRFEFeatureSelection (Xt, Yt, columnValues, assetName) :\n",
        "    \n",
        "    rfe_method_lasso = RFE(Lasso(), n_features_to_select=100, step=1) # we will select 100 features\n",
        "    rfe_method_lasso.fit(Xt,Yt)\n",
        "\n",
        "    rfe_method_ridge = RFE(Ridge(), n_features_to_select=100, step=1) # we will select 100 features\n",
        "    rfe_method_ridge.fit(Xt,Yt)\n",
        "\n",
        "    rfe_method_linear = RFE(LinearRegression(), n_features_to_select=100, step=1) # we will select 4 features\n",
        "    rfe_method_linear.fit(Xt,Yt)\n",
        "\n",
        "    df_rfe = pd.DataFrame()\n",
        "    df_rfe ['feature_names'] = columnValues\n",
        "    df_rfe ['RFE Selection Lasso'] = pd.DataFrame(rfe_method_lasso.support_)\n",
        "    df_rfe ['RFE Selection Ridge'] = pd.DataFrame(rfe_method_ridge.support_)\n",
        "    df_rfe ['RFE Selection Linear Regression'] = pd.DataFrame(rfe_method_linear.support_)\n",
        "    \n",
        "    print(\"RFE features for Data : \", assetName, \" : \")\n",
        "    print(df_rfe)\n",
        "       \n",
        "    return df_rfe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8_kz1x6zfwj7"
      },
      "outputs": [],
      "source": [
        "# Feature selection using RFE - Ridge, Lasso, Linear Regression\n",
        "from sklearn.feature_selection import RFECV, RFE\n",
        "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
        "\n",
        "SPY_selectRidgeFeaturesList = runRFEFeatureSelection (SPY_feature_Xt, SPY_feature_Yt, fe_df_SPY.columns.values, \"SPY\")\n",
        "DIA_selectRidgeFeaturesList = runRFEFeatureSelection (DIA_feature_Xt, DIA_feature_Yt, fe_df_DIA.columns.values, \"DIA\")\n",
        "QQQ_selectRidgeFeaturesList = runRFEFeatureSelection (QQQ_feature_Xt, QQQ_feature_Yt, fe_df_QQQ.columns.values, \"QQQ\")\n",
        "\n",
        "writeCSV(SPY_selectRidgeFeaturesList, \"SPY_selectRidgeFeaturesList\")\n",
        "writeCSV(DIA_selectRidgeFeaturesList, \"DIA_selectRidgeFeaturesList\")\n",
        "writeCSV(QQQ_selectRidgeFeaturesList, \"QQQ_selectRidgeFeaturesList\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7CcDtHyNfyl-"
      },
      "outputs": [],
      "source": [
        "feature_collection_list_SPY.append(SPY_selectRidgeFeaturesList[(SPY_selectRidgeFeaturesList['RFE Selection Lasso']==True)]['feature_names'].values.tolist())\n",
        "feature_collection_list_SPY.append(SPY_selectRidgeFeaturesList[(SPY_selectRidgeFeaturesList['RFE Selection Ridge']==True)]['feature_names'].values.tolist())\n",
        "feature_collection_list_SPY.append(SPY_selectRidgeFeaturesList[(SPY_selectRidgeFeaturesList['RFE Selection Linear Regression']==True)]['feature_names'].values.tolist())\n",
        "\n",
        "\n",
        "feature_collection_list_DIA.append(DIA_selectRidgeFeaturesList[(DIA_selectRidgeFeaturesList['RFE Selection Lasso']==True)]['feature_names'].values.tolist())\n",
        "feature_collection_list_DIA.append(DIA_selectRidgeFeaturesList[(DIA_selectRidgeFeaturesList['RFE Selection Ridge']==True)]['feature_names'].values.tolist())\n",
        "feature_collection_list_DIA.append(DIA_selectRidgeFeaturesList[(DIA_selectRidgeFeaturesList['RFE Selection Linear Regression']==True)]['feature_names'].values.tolist())\n",
        "\n",
        "feature_collection_list_QQQ.append(QQQ_selectRidgeFeaturesList[(QQQ_selectRidgeFeaturesList['RFE Selection Lasso']==True)]['feature_names'].values.tolist())\n",
        "feature_collection_list_QQQ.append(QQQ_selectRidgeFeaturesList[(QQQ_selectRidgeFeaturesList['RFE Selection Ridge']==True)]['feature_names'].values.tolist())\n",
        "feature_collection_list_QQQ.append(QQQ_selectRidgeFeaturesList[(QQQ_selectRidgeFeaturesList['RFE Selection Linear Regression']==True)]['feature_names'].values.tolist())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oFMY4ThQsGTV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-aeMSsqDf1DL"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Feature Selection - SELECT K-BEST\n",
        "from sklearn.feature_selection import f_regression, SelectKBest, SelectPercentile\n",
        "\n",
        "SPY_selectKFeaturesList = runSelectKFeatures (f_regression, SPY_feature_Xt, SPY_feature_Yt, fe_df_SPY.columns.values, \"SPY\")\n",
        "DIA_selectKFeaturesList = runSelectKFeatures (f_regression, DIA_feature_Xt, DIA_feature_Yt, fe_df_DIA.columns.values, \"DIA\")\n",
        "QQQ_selectKFeaturesList = runSelectKFeatures (f_regression, QQQ_feature_Xt, QQQ_feature_Yt, fe_df_QQQ.columns.values, \"QQQ\")\n",
        "\n",
        "writeCSV(pd.DataFrame(SPY_selectKFeaturesList), \"SPY_selectKFeaturesList\")\n",
        "writeCSV(pd.DataFrame(DIA_selectKFeaturesList), \"DIA_selectKFeaturesList\")\n",
        "writeCSV(pd.DataFrame(QQQ_selectKFeaturesList), \"QQQ_selectKFeaturesList\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "xLpMWvRRf26k",
        "outputId": "1c4b1a51-61a5-4c9b-b1fb-983ad49ab617"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['ADOSC_3_10', 'ADX_14', 'AR_26', 'BR_26', 'CDL_2CROWS', 'CDL_3BLACKCROWS', 'CDL_3INSIDE', 'CDL_3LINESTRIKE', 'CDL_3OUTSIDE', 'CDL_3WHITESOLDIERS', 'CDL_ADVANCEBLOCK', 'CDL_BELTHOLD', 'CDL_CLOSINGMARUBOZU', 'CDL_COUNTERATTACK', 'CDL_DARKCLOUDCOVER', 'CDL_DOJISTAR', 'CDL_ENGULFING', 'CDL_EVENINGDOJISTAR', 'CDL_EVENINGSTAR', 'CDL_GAPSIDESIDEWHITE', 'CDL_GRAVESTONEDOJI', 'CDL_HAMMER', 'CDL_HANGINGMAN', 'CDL_HARAMI', 'CDL_HARAMICROSS', 'CDL_HIGHWAVE', 'CDL_HIKKAKE', 'CDL_HIKKAKEMOD', 'CDL_HOMINGPIGEON', 'CDL_IDENTICAL3CROWS', 'CDL_INNECK', 'CDL_INSIDE', 'CDL_INVERTEDHAMMER', 'CDL_LADDERBOTTOM', 'CDL_LONGLINE', 'CDL_MARUBOZU', 'CDL_MATCHINGLOW', 'CDL_MORNINGDOJISTAR', 'CDL_MORNINGSTAR', 'CDL_ONNECK', 'CDL_PIERCING', 'CDL_RICKSHAWMAN', 'CDL_SEPARATINGLINES', 'CDL_SHOOTINGSTAR', 'CDL_SHORTLINE', 'CDL_SPINNINGTOP', 'CDL_STALLEDPATTERN', 'CDL_STICKSANDWICH', 'CDL_TASUKIGAP', 'CDL_THRUSTING', 'CDL_TRISTAR', 'CDL_UNIQUE3RIVER', 'CDL_UPSIDEGAP2CROWS', 'CDL_XSIDEGAP3METHODS', 'CHOP_14_1_100', 'DPO_20', 'EOM_14_100000000', 'ER_10', 'KURT_30', 'KVOs_34_55_13', 'MASSI_9_25', 'MFI_14', 'PSARaf_0.02_0.2', 'PSARr_0.02_0.2', 'PVR', 'QQEl_14_5_4.236', 'QQEs_14_5_4.236', 'QS_10', 'SKEW_30', 'SQZPRO_ON_NARROW', 'STC_10_12_26_0.5', 'STCstoch_10_12_26_0.5', 'SUPERTd_7_3.0', 'THERMO_20_2_0.5', 'THERMOl_20_2_0.5', 'THERMOs_20_2_0.5', 'TTM_TRND_6', 'VHF_28', 'MEDCPIM158SFRBCLE'], ['PPOh_12_26_9', 'PPOs_12_26_9', 'PSARl_0.02_0.2', 'PSARs_0.02_0.2', 'PSARaf_0.02_0.2', 'PSARr_0.02_0.2', 'PSL_12', 'PVI_1', 'PVO_12_26_9', 'PVOh_12_26_9', 'PVOs_12_26_9', 'PVOL', 'PVR', 'PVT', 'PWMA_10', 'QQE_14_5_4.236', 'QQE_14_5_4.236_RSIMA', 'QQEl_14_5_4.236', 'QQEs_14_5_4.236', 'QS_10', 'QTL_30_0.5', 'RMA_10', 'ROC_10', 'RSI_14', 'RSX_14', 'RVGI_14_4', 'RVGIs_14_4', 'RVI_14', 'SINWMA_14', 'SKEW_30', 'SLOPE_1', 'SMA_10', 'SMI_5_20_5', 'SMIs_5_20_5', 'SMIo_5_20_5', 'SQZ_20_2.0_20_1.5', 'SQZ_ON', 'SQZ_OFF', 'SQZ_NO', 'SQZPRO_20_2.0_20_2_1.5_1', 'SQZPRO_ON_WIDE', 'SQZPRO_ON_NORMAL', 'SQZPRO_ON_NARROW', 'SQZPRO_OFF', 'SQZPRO_NO', 'SSF_10_2', 'STC_10_12_26_0.5', 'STCmacd_10_12_26_0.5', 'STCstoch_10_12_26_0.5', 'STDEV_30', 'STOCHk_14_3_3', 'STOCHd_14_3_3', 'STOCHRSIk_14_14_3_3', 'STOCHRSId_14_14_3_3', 'SUPERT_7_3.0', 'SUPERTd_7_3.0', 'SUPERTl_7_3.0', 'SUPERTs_7_3.0', 'SWMA_10', 'T3_10_0.7', 'TEMA_10', 'THERMO_20_2_0.5', 'THERMOma_20_2_0.5', 'THERMOl_20_2_0.5', 'THERMOs_20_2_0.5', 'TOS_STDEVALL_LR', 'TOS_STDEVALL_L_1', 'TOS_STDEVALL_U_1', 'TOS_STDEVALL_L_2', 'TOS_STDEVALL_U_2', 'TOS_STDEVALL_L_3', 'TOS_STDEVALL_U_3', 'TRIMA_10', 'TRIX_30_9', 'TRIXs_30_9', 'TRUERANGE_1', 'TSI_13_25_13', 'TSIs_13_25_13', 'TTM_TRND_6', 'UI_14', 'UO_7_14_28', 'VAR_30', 'VHF_28', 'VIDYA_14', 'VTXP_14', 'VTXM_14', 'VWAP_D', 'VWMA_10', 'WCP', 'WILLR_14', 'WMA_10', 'ZL_EMA_10', 'ZS_30', 'GDP', 'MEDCPIM158SFRBCLE', 'UNRATE', 'MORTGAGE30US', 'VIXCLS', 'TB3MS', 'REAINTRATREARAT10Y'], ['High', 'Low', 'Close', 'Adj Close', 'Volume', 'ACCBL_20', 'ACCBM_20', 'ACCBU_20', 'DMP_14', 'DMN_14', 'ALMA_10_6.0_0.85', 'AO_5_34', 'OBVe_12', 'low_Z_30_1', 'CKSPl_10_3_20', 'CMO_14', 'LDECAY_5', 'DEMA_10', 'DCL_20_20', 'DCM_20_20', 'DCU_20_20', 'EMA_10', 'FWMA_10', 'HA_high', 'HA_low', 'HA_close', 'HILO_13_21', 'HILOl_13_21', 'HILOs_13_21', 'HL2', 'HLC3', 'HWM', 'HWU', 'HWL', 'HWMA_0.2_0.1_0.1', 'ISB_26', 'IKS_26', 'ICS_26', 'KAMA_10_2_30', 'KCLe_20_2', 'KCBe_20_2', 'KCUe_20_2', 'KST_10_15_20_30_10_10_10_15', 'LOGRET_1', 'MACD_12_26_9', 'MACDh_12_26_9', 'MAD_30', 'MCGD_10', 'MEDIAN_30', 'MIDPOINT_2', 'MIDPRICE_2', 'NVI_1', 'OHLC4', 'PCTRET_1', 'PPOs_12_26_9', 'PSARl_0.02_0.2', 'PVI_1', 'PVOs_12_26_9', 'PVOL', 'PVT', 'QTL_30_0.5', 'RMA_10', 'SINWMA_14', 'SLOPE_1', 'SMA_10', 'SMI_5_20_5', 'SQZ_20_2.0_20_1.5', 'STCmacd_10_12_26_0.5', 'STDEV_30', 'SUPERT_7_3.0', 'SUPERTl_7_3.0', 'SUPERTs_7_3.0', 'T3_10_0.7', 'TEMA_10', 'THERMOma_20_2_0.5', 'TOS_STDEVALL_LR', 'TOS_STDEVALL_L_1', 'TOS_STDEVALL_U_1', 'TOS_STDEVALL_L_2', 'TOS_STDEVALL_U_2', 'TOS_STDEVALL_L_3', 'TOS_STDEVALL_U_3', 'TRIX_30_9', 'TRIXs_30_9', 'TSI_13_25_13', 'TSIs_13_25_13', 'UI_14', 'VIDYA_14', 'VWAP_D', 'VWMA_10', 'WCP', 'WMA_10', 'ZL_EMA_10', 'ZS_30', 'GDP', 'UNRATE', 'MORTGAGE30US', 'VIXCLS', 'TB3MS', 'REAINTRATREARAT10Y'], ['Open', 'High', 'Low', 'Close', 'Adj Close', 'ABER_ZG_5_15', 'ABER_SG_5_15', 'ABER_XG_5_15', 'ABER_ATR_5_15', 'ACCBL_20', 'ACCBM_20', 'ACCBU_20', 'AD', 'DMN_14', 'ALMA_10_6.0_0.85', 'AMATe_LR_8_21_2', 'AMATe_SR_8_21_2', 'OBV', 'OBV_min_2', 'OBV_max_2', 'OBVe_4', 'OBVe_12', 'AOBV_LR_2', 'AOBV_SR_2', 'APO_12_26', 'ATRr_14', 'BBL_5_2.0', 'BBM_5_2.0', 'BBU_5_2.0', 'low_Z_30_1', 'close_Z_30_1', 'CKSPl_10_3_20', 'DEMA_10', 'DCL_20_20', 'DCM_20_20', 'DCU_20_20', 'EMA_10', 'BULLP_13', 'BEARP_13', 'FWMA_10', 'HA_open', 'HA_high', 'HA_low', 'HA_close', 'HILO_13_21', 'HILOs_13_21', 'HL2', 'HLC3', 'HMA_10', 'HWM', 'ISA_9', 'ISB_26', 'ITS_9', 'IKS_26', 'JMA_7_0', 'KAMA_10_2_30', 'KCLe_20_2', 'KCBe_20_2', 'KCUe_20_2', 'LR_14', 'MACD_12_26_9', 'MACDs_12_26_9', 'MCGD_10', 'MEDIAN_30', 'MIDPOINT_2', 'MIDPRICE_2', 'OHLC4', 'PSARl_0.02_0.2', 'PSARs_0.02_0.2', 'PWMA_10', 'QTL_30_0.5', 'RMA_10', 'SINWMA_14', 'SLOPE_1', 'SMA_10', 'SMI_5_20_5', 'SMIs_5_20_5', 'SMIo_5_20_5', 'SQZPRO_ON_NORMAL', 'SSF_10_2', 'STCmacd_10_12_26_0.5', 'SUPERT_7_3.0', 'SUPERTl_7_3.0', 'SWMA_10', 'T3_10_0.7', 'TEMA_10', 'TOS_STDEVALL_LR', 'TOS_STDEVALL_L_1', 'TOS_STDEVALL_U_1', 'TOS_STDEVALL_L_2', 'TOS_STDEVALL_U_2', 'TOS_STDEVALL_L_3', 'TOS_STDEVALL_U_3', 'TRIMA_10', 'VWAP_D', 'VWMA_10', 'WCP', 'WMA_10', 'ZL_EMA_10', 'ZS_30'], ['Open', 'High', 'Low', 'Close', 'Adj Close', 'ABER_ZG_5_15', 'ABER_SG_5_15', 'ABER_XG_5_15', 'ABER_ATR_5_15', 'ACCBL_20', 'ACCBM_20', 'ACCBU_20', 'AD', 'DMP_14', 'ALMA_10_6.0_0.85', 'OBV', 'OBV_min_2', 'OBV_max_2', 'OBVe_4', 'OBVe_12', 'ATRr_14', 'BBL_5_2.0', 'BBM_5_2.0', 'BBU_5_2.0', 'CKSPl_10_3_20', 'CKSPs_10_3_20', 'LDECAY_5', 'DEMA_10', 'DCL_20_20', 'DCM_20_20', 'DCU_20_20', 'EMA_10', 'FWMA_10', 'HA_open', 'HA_high', 'HA_low', 'HA_close', 'HILO_13_21', 'HILOl_13_21', 'HILOs_13_21', 'HL2', 'HLC3', 'HMA_10', 'HWM', 'HWU', 'HWL', 'HWMA_0.2_0.1_0.1', 'ISA_9', 'ISB_26', 'ITS_9', 'IKS_26', 'ICS_26', 'JMA_7_0', 'KAMA_10_2_30', 'KCLe_20_2', 'KCBe_20_2', 'KCUe_20_2', 'LR_14', 'MAD_30', 'MCGD_10', 'MEDIAN_30', 'MIDPOINT_2', 'MIDPRICE_2', 'NVI_1', 'OHLC4', 'PSARl_0.02_0.2', 'PSARs_0.02_0.2', 'PVI_1', 'PVT', 'PWMA_10', 'QTL_30_0.5', 'RMA_10', 'SINWMA_14', 'SMA_10', 'SSF_10_2', 'STDEV_30', 'SUPERT_7_3.0', 'SUPERTl_7_3.0', 'SUPERTs_7_3.0', 'SWMA_10', 'T3_10_0.7', 'TEMA_10', 'THERMOma_20_2_0.5', 'TOS_STDEVALL_LR', 'TOS_STDEVALL_L_1', 'TOS_STDEVALL_U_1', 'TOS_STDEVALL_L_2', 'TOS_STDEVALL_U_2', 'TOS_STDEVALL_L_3', 'TOS_STDEVALL_U_3', 'TRIMA_10', 'VIDYA_14', 'VWAP_D', 'VWMA_10', 'WCP', 'WMA_10', 'ZL_EMA_10', 'GDP', 'MORTGAGE30US', 'REAINTRATREARAT10Y']]\n"
          ]
        }
      ],
      "source": [
        "feature_collection_list_SPY.append(SPY_selectKFeaturesList)\n",
        "feature_collection_list_DIA.append(DIA_selectKFeaturesList)\n",
        "feature_collection_list_QQQ.append(QQQ_selectKFeaturesList)\n",
        "print(feature_collection_list_SPY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wbTuG676gB03"
      },
      "outputs": [],
      "source": [
        "# ---------------------------------------------------------------------------------------------------------BORUTA FEATUE ENGINEERING------------------------------------------------------------------------\n",
        "# Feature selection using Boruta\n",
        "\n",
        "def runBorutaFeatureSelection (features, Xt, Yt, noEstimators, assetName) : # pass transformed training set as well as target set\n",
        "    \n",
        "    print(\"BORUTA FOR ASSET : \", assetName, \"....Estimators : \", noEstimators)\n",
        "    \n",
        "    rfc = RandomForestRegressor(n_estimators=noEstimators, n_jobs=1000, max_depth=10)\n",
        "    boruta_selector = BorutaPy(rfc, n_estimators=noEstimators)        \n",
        "    print(\"Running BORUTA : \", assetName)    \n",
        "    boruta_selector.fit(Xt, Yt)\n",
        "   \n",
        "       \n",
        "    # number of selected features\n",
        "    print ('\\n Number of selected features:')\n",
        "    print (boruta_selector.n_features_)\n",
        "    feature_df = pd.DataFrame(features.tolist(), columns=['features'])\n",
        "    feature_df['rank']=boruta_selector.ranking_\n",
        "    feature_df = feature_df.sort_values('rank', ascending=True).reset_index(drop=True)\n",
        "    print ('\\n Top %d features:' % boruta_selector.n_features_)\n",
        "    print (feature_df.head(boruta_selector.n_features_))\n",
        "    writeCSV(feature_df, assetName+\"_\"+\"select_borutaFeatureDF\")\n",
        "    #feature_df.to_csv('/content/drive/MyDrive/Colab Notebooks/output data/'+assetName+'_'+'boruta-feature-ranking.csv', index=False)\n",
        "\n",
        "    # check ranking of features\n",
        "    print ('\\n Feature ranking:')\n",
        "    print (boruta_selector.ranking_)\n",
        "    return feature_df.head(boruta_selector.n_features_)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nwC5qJyGgDr8"
      },
      "outputs": [],
      "source": [
        "# Feature selection using Boruta\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from boruta import BorutaPy\n",
        "\n",
        "numner_of_Estimators = 200 # this is an optimal number given that there is marginal benefit for the increase in computational power by increasing trees\n",
        "\n",
        "SPY_boruta_features = runBorutaFeatureSelection (fe_df_SPY.columns,SPY_feature_Xt, SPY_feature_Yt, numner_of_Estimators, \"SPY\")\n",
        "writeCSV(SPY_boruta_features, \"SPY_boruta_features_FULL\")\n",
        "\n",
        "\n",
        "DIA_boruta_features = runBorutaFeatureSelection (fe_df_DIA.columns,DIA_feature_Xt, DIA_feature_Yt, numner_of_Estimators, \"DIA\")\n",
        "writeCSV(DIA_boruta_features, \"DIA_boruta_features_FULL\")\n",
        "\n",
        "QQQ_boruta_featues  = runBorutaFeatureSelection (fe_df_QQQ.columns,QQQ_feature_Xt, QQQ_feature_Yt, numner_of_Estimators, \"QQQ\")\n",
        "writeCSV(QQQ_boruta_features, \"QQQ_boruta_features_FULL\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iCw4vp8ZgFY1"
      },
      "outputs": [],
      "source": [
        "#load boruta features\n",
        "\n",
        "feature_collection_list_SPY.append(SPY_boruta_features['features'].values.tolist())\n",
        "feature_collection_list_DIA.append(DIA_boruta_features['features'].values.tolist())\n",
        "feature_collection_list_QQQ.append(QQQ_boruta_features['features'].values.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nxE9wTuJgOXH"
      },
      "outputs": [],
      "source": [
        "# ---------------------------------------------------------------------------------------------------------DATA PRUNING STEP------------------------------------------------------------------------\n",
        "# prepare the data cut with shortlisted features for all the asset types\n",
        "# all the features extracted by methods above have been written to csv. We will prepare the final list of all features for all assets and load it here to create a final data set\n",
        "\n",
        "final_SPY_features = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/input data/final_SPY_features.csv\", header=None)\n",
        "final_DIA_features = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/input data/final_DIA_features.csv\", header=None)\n",
        "final_QQQ_features = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/input data/final_QQQ_features.csv\", header=None)\n",
        "\n",
        "final_SPY_rawData = pd.DataFrame()\n",
        "final_DIA_rawData = pd.DataFrame()\n",
        "final_QQQ_rawData = pd.DataFrame()\n",
        "\n",
        "print(\"SPY final features count : \", len(final_SPY_features))\n",
        "print(\"DIA final features count : \", len(final_DIA_features))\n",
        "print(\"QQQ final features count : \", len(final_QQQ_features))\n",
        "\n",
        "spy_list = ['PSARl_0.02_0.2','QTL_30_0.5','RMA_10','SINWMA_14','SMA_10','SUPERT_7_3.0','SUPERTl_7_3.0','T3_10_0.7','TEMA_10','TOS_STDEVALL_LR','TOS_STDEVALL_L_1','TOS_STDEVALL_U_1','TOS_STDEVALL_L_2','TOS_STDEVALL_U_2','TOS_STDEVALL_L_3','TOS_STDEVALL_U_3','VWAP_D','VWMA_10','WCP','WMA_10','ZL_EMA_10','High','Low','Close','Adj Close','ACCBL_20','ACCBM_20','ACCBU_20','ALMA_10_6.0_0.85','OBVe_12','CKSPl_10_3_20','DEMA_10','DCM_20_20','DCU_20_20','EMA_10','FWMA_10','HA_high','HA_low','HA_close','HILO_13_21','HL2','HLC3','HWM','ISB_26','IKS_26','KAMA_10_2_30','KCLe_20_2','KCBe_20_2','KCUe_20_2','MCGD_10','MEDIAN_30','MIDPOINT_2','MIDPRICE_2','OHLC4','PVI_1','PVT','PWMA_10','SSF_10_2','SWMA_10','TRIMA_10','VIDYA_14','MORTGAGE30US','REAINTRATREARAT10Y','Open','ABER_ZG_5_15','ABER_SG_5_15','ABER_XG_5_15','AD','OBV','OBV_min_2','OBV_max_2','OBVe_4','BBL_5_2.0','BBM_5_2.0','BBU_5_2.0','LDECAY_5','DCL_20_20','HA_open','HILOl_13_21','HILOs_13_21','HMA_10','HWU','HWL','HWMA_0.2_0.1_0.1','ITS_9','JMA_7_0','LR_14','NVI_1','PSARs_0.02_0.2','QS_10','SLOPE_1','SMI_5_20_5','STCmacd_10_12_26_0.5','STDEV_30','SUPERTs_7_3.0','THERMOma_20_2_0.5','ZS_30','GDP','UNRATE','VIXCLS']\n",
        "final_SPY_rawData= fe_df_SPY[spy_list] \n",
        "\n",
        "DIA_list = ['SUPERTl_7_3.0','T3_10_0.7','TEMA_10','TOS_STDEVALL_U_2','TOS_STDEVALL_L_3','TOS_STDEVALL_U_3','VWAP_D','WCP','ZL_EMA_10','ACCBL_20','ACCBM_20','ALMA_10_6.0_0.85','ABER_ZG_5_15','ABER_SG_5_15','OBV_min_2','BBL_5_2.0','BBM_5_2.0','BBU_5_2.0','QTL_30_0.5','RMA_10','SINWMA_14','TOS_STDEVALL_LR','TOS_STDEVALL_L_1','TOS_STDEVALL_U_1','TOS_STDEVALL_L_2','VWMA_10','WMA_10','High','Low','Close','Adj Close','ACCBU_20','OBVe_12','DEMA_10','DCM_20_20','EMA_10','FWMA_10','HA_high','HA_low','HA_close','HL2','HLC3','IKS_26','KAMA_10_2_30','KCLe_20_2','KCBe_20_2','KCUe_20_2','MCGD_10','MEDIAN_30','MIDPOINT_2','MIDPRICE_2','OHLC4','PVI_1','SSF_10_2','SWMA_10','TRIMA_10','VIDYA_14','MORTGAGE30US','REAINTRATREARAT10Y','ABER_XG_5_15','AD','OBV','OBV_max_2','OBVe_4','LDECAY_5','HMA_10','HWU','HWL','JMA_7_0','NVI_1','ABER_ATR_5_15','ATRr_14','CKSPs_10_3_20','PSARl_0.02_0.2','SMA_10','SUPERT_7_3.0','CKSPl_10_3_20','DCU_20_20','HILO_13_21','HWM','ISB_26','PVT','PWMA_10','Open','DCL_20_20','HA_open','HILOl_13_21','ITS_9','LR_14','PSARs_0.02_0.2','SUPERTs_7_3.0','THERMOma_20_2_0.5','GDP','UNRATE','DMP_14','AR_26','ISA_9','ICS_26','TRIX_30_9','TRIXs_30_9','ADX_14','AMATe_LR_8_21_2','AMATe_SR_8_21_2']\n",
        "final_DIA_rawData= fe_df_DIA[DIA_list] \n",
        "\n",
        "QQQ_list=['T3_10_0.7','TEMA_10','TOS_STDEVALL_U_2','TOS_STDEVALL_L_3','TOS_STDEVALL_U_3','VWAP_D','WCP','ZL_EMA_10','ACCBL_20','ACCBM_20','ALMA_10_6.0_0.85','ABER_ZG_5_15','ABER_SG_5_15','OBV_min_2','BBL_5_2.0','BBM_5_2.0','BBU_5_2.0','TOS_STDEVALL_LR','TOS_STDEVALL_U_1','TOS_STDEVALL_L_2','VWMA_10','WMA_10','Adj Close','ACCBU_20','SWMA_10','TRIMA_10','ABER_XG_5_15','OBVe_4','SUPERT_7_3.0','SUPERTl_7_3.0','RMA_10','SINWMA_14','TOS_STDEVALL_L_1','High','Low','Close','OBVe_12','DEMA_10','EMA_10','FWMA_10','HA_high','HA_low','HA_close','HL2','HLC3','IKS_26','KCLe_20_2','KCBe_20_2','KCUe_20_2','MCGD_10','MEDIAN_30','MIDPOINT_2','MIDPRICE_2','OHLC4','SSF_10_2','VIDYA_14','AD','OBV','LDECAY_5','HWL','JMA_7_0','ABER_ATR_5_15','ATRr_14','SMA_10','CKSPl_10_3_20','HWM','ISB_26','Open','ISA_9','QTL_30_0.5','DCM_20_20','PVI_1','HMA_10','HWU','NVI_1','CKSPs_10_3_20','PSARl_0.02_0.2','DCU_20_20','HILO_13_21','PVT','PWMA_10','DCL_20_20','HA_open','HILOl_13_21','LR_14','SUPERTs_7_3.0','GDP','DMP_14','HWMA_0.2_0.1_0.1','MEDCPIM158SFRBCLE','TB3MS','ADOSC_3_10','AO_5_34','AOBV_LR_2','AOBV_SR_2']\n",
        "final_QQQ_rawData= fe_df_QQQ[QQQ_list] \n",
        "\n",
        "\n",
        "print(\"SPY final raw data shape for deep learning : \", final_SPY_rawData.shape)\n",
        "print(\"DIA final raw data shape for deep learning : \", final_DIA_rawData.shape)\n",
        "print(\"QQQ final raw data shape for deep learning : \", final_QQQ_rawData.shape)\n",
        "\n",
        "\n",
        "\n",
        "writeCSV(final_SPY_rawData, \"SPY_feature_selected_final_rawData\")\n",
        "writeCSV(final_DIA_rawData, \"DIA_feature_selected_final_rawData\")\n",
        "writeCSV(final_QQQ_rawData, \"QQQ_feature_selected_final_rawData\")\n",
        "\n",
        "\n",
        "print(final_SPY_rawData)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hpLEGubdgQEM"
      },
      "outputs": [],
      "source": [
        "#--------------------------------------------------------------------------------------DEEP LEARNING --------------------------------------------------------------------------------------------------------------------------#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tJquIpolCjuz"
      },
      "outputs": [],
      "source": [
        "# ---------------------------------------------------------------------------------------------------------DATA SCALE AND SPLIT FOR DEEP LEARNING------------------------------------------------------------------------\n",
        "# define function to scale, split data into training set and test set\n",
        "\n",
        "def scaleAndTrainTestSplit(data_X, data_Y, scaler, assetName) :\n",
        "  \n",
        "  # we require different scalers for X and Y because of the dimensionality. data_x is the training data set with multiple features, data_y is the target training data set with one feature\n",
        "  if(scaler=='StandardScaler'):\n",
        "    scaler_x = StandardScaler()\n",
        "    scaler_y = StandardScaler()\n",
        "\n",
        "  if(scaler=='MinMax'):\n",
        "    scaler_x = MinMaxScaler()\n",
        "    scaler_y = MinMaxScaler()\n",
        "\n",
        "  feature_X_train, feature_X_test = train_test_split(data_X, test_size=0.2, random_state=0, shuffle=False)\n",
        "  feature_Y_train, feature_Y_test = train_test_split(data_Y, test_size=0.2, random_state=0, shuffle=False)\n",
        "\n",
        "  scaled_feature_X_train = scaler_x.fit_transform(feature_X_train)\n",
        "  scaled_feature_X_test = scaler_x.transform(feature_X_test)\n",
        "\n",
        "  scaled_feature_Y_train = scaler_y.fit_transform(feature_Y_train)\n",
        "  scaled_feature_Y_test = scaler_y.transform(feature_Y_test)\n",
        "\n",
        "  print(\"X-train shape: \", scaled_feature_X_train.shape)\n",
        "  print(\"X-test  shape: \", scaled_feature_X_test.shape)\n",
        "\n",
        "  print(\"Y-train shape: \", scaled_feature_Y_train.shape)\n",
        "  print(\"Y-test  shape: \", scaled_feature_Y_test.shape)\n",
        "\n",
        "  return scaled_feature_X_train, scaled_feature_X_test, scaled_feature_Y_train, scaled_feature_Y_test, scaler_x, scaler_y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v3TfdzAPJ-tK"
      },
      "outputs": [],
      "source": [
        "# ---------------------------------------------------------------------------------------------------------DATA SEQUENCING------------------------------------------------------------------------\n",
        "# sequence split for Deep Learning \n",
        "def generate_sequence(data_X, data_Y, sequence_length):\n",
        "    \n",
        "    # create X & y data array\n",
        "    X = []\n",
        "    Y = []\n",
        "    \n",
        "    for i in range (sequence_length, len(data_X)) :\n",
        "        X.append(data_X[i- sequence_length : i])\n",
        "        Y.append(data_Y[i])    \n",
        "      \n",
        "    return np.array(X), np.array(Y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pjmEkNPzS18Y"
      },
      "outputs": [],
      "source": [
        "# ---------------------------------------------------------------------------------------------------------STACKED LSTM MODEL COMPILATION------------------------------------------------------------------------\n",
        "def create_model_stackedLSTM(hu, lookback,ipShape_features):\n",
        "    print(\"hu value :\", hu, \" lookback : \", lookback)\n",
        "    tensorflow.keras.backend.clear_session()   \n",
        "    \n",
        "    # instantiate the model\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(units=hu, input_shape=(lookback, ipShape_features), activation = 'elu', return_sequences=True, name='LSTM1'))\n",
        "    model.add(Dropout(0.25)) # hypertuned dropoutrate\n",
        "    model.add(LSTM(units=hu, activation = 'relu', return_sequences=True, name='LSTM2'))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(LSTM(units=hu, activation = 'relu', return_sequences=False, name='LSTM3'))    \n",
        "    model.add(Dense(units=1, name='Output'))              # can also specify linear activation function \n",
        "    \n",
        "    # specify optimizer separately (preferred method))\n",
        "    #opt = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n",
        "    opt = Adam(lr=0.001, epsilon=1e-08, decay=0.0)       # adam optimizer seems to perform better for a single lstm\n",
        "    \n",
        "    # model compilation\n",
        "    model.compile(optimizer=opt, loss='mse', metrics=['mae'])\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4N3qpWGeTBDP"
      },
      "outputs": [],
      "source": [
        "# ---------------------------------------------------------------------------------------------------------SINGLE LSTM MODEL COMPILATION------------------------------------------------------------------------\n",
        "def create_model_singleLSTM(hu, lookback, ipShape_features):\n",
        "    print(\"hu value :\", hu, \" lookback : \", lookback)\n",
        "    tensorflow.keras.backend.clear_session()   \n",
        "    \n",
        "    # instantiate the model\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(units=hu, input_shape=(lookback, ipShape_features), activation = 'relu', return_sequences=False, name='LSTM'))\n",
        "    model.add(Dense(units=1, name='Output'))              # can also specify linear activation function \n",
        "    \n",
        "    # specify optimizer separately (preferred method))\n",
        "    #opt = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n",
        "    opt = Adam(lr=0.001, epsilon=1e-08, decay=0.0)       # adam optimizer seems to perform better for a single lstm\n",
        "    \n",
        "    # model compilation\n",
        "    model.compile(optimizer=opt, loss='mse', metrics=['mae'])\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "20sPmHuKTVq_"
      },
      "outputs": [],
      "source": [
        "# ---------------------------------------------------------------------------------------------------------MODEL EXECUTION------------------------------------------------------------------------\n",
        "def callModel(scaled_X_train, scaled_Y_train, scaled_X_test, scaled_Y_test, scaler_y, modelType, hu, lookback, ipShape_features, patience, epochs, batchsize, assetName) :\n",
        "\n",
        "  print(\"Asset Type: \", assetName, \"...Calling model: \", modelType)\n",
        "  \n",
        "  X_train, Y_train = generate_sequence(scaled_X_train, scaled_Y_train, sequence_length=lookback)\n",
        "  X_test, Y_test = generate_sequence(scaled_X_test, scaled_Y_test, sequence_length=lookback)\n",
        "\n",
        "  if(modelType=='singleLSTM'):\n",
        "    model = create_model_singleLSTM(hu, lookback, ipShape_features)\n",
        "  \n",
        "  if(modelType=='stackedLSTM'):\n",
        "    model = create_model_stackedLSTM(hu, lookback,ipShape_features)\n",
        "  \n",
        "  # summary\n",
        "  model.summary()\n",
        "\n",
        "  model_path = (assetName+'_'+modelType+'_'+'hu_'+str(hu)+'_'+'model.h5')\n",
        "  logdir = os.path.join(model_path+\"_\"+\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "  %tensorboard --logdir logs\n",
        "\n",
        "  my_callbacks = [\n",
        "    EarlyStopping(patience=patience, monitor='loss', mode='min', verbose=1, restore_best_weights=True),\n",
        "    ModelCheckpoint(filepath=model_path, verbose=1, monitor='loss', save_best_only=True),\n",
        "    TensorBoard(log_dir=logdir, histogram_freq=1)\n",
        "  ]\n",
        "\n",
        "  # Model fitting\n",
        "  lstm_training = model.fit(X_train, \n",
        "                            Y_train, \n",
        "                            batch_size=batchsize, \n",
        "                            epochs=epochs, \n",
        "                            verbose=0, \n",
        "                            callbacks=my_callbacks, \n",
        "                            shuffle=False)\n",
        "  print(logdir)\n",
        "  \n",
        "\n",
        "  # calculate rmse of loss function\n",
        "  train_rmse_scaled = np.sqrt(model.evaluate(X_train, Y_train, verbose=0))\n",
        "  test_rmse_scaled = np.sqrt(model.evaluate(X_test, Y_test, verbose=0))\n",
        "  print(f'Train RMSE: {train_rmse_scaled[0]:.4f} | Test RMSE: {test_rmse_scaled[0]:.4f}')\n",
        "\n",
        "  # predictions\n",
        "  Y_pred = model.predict(X_test)\n",
        "  print(\"Y prediction shape : \", Y_pred.shape)\n",
        "\n",
        "  print(\"Comparison between actual and predicted values for asset \", assetName, \" : \")\n",
        "\n",
        "  df_compare = pd.DataFrame({\n",
        "    'actual': scaler_y.inverse_transform(Y_test).flatten(),\n",
        "    'prediction': scaler_y.inverse_transform(Y_pred).flatten()})\n",
        "    #index = y_test[lookback:].index)\n",
        "\n",
        "  df_compare['spread'] = df_compare['prediction'] - df_compare['actual']\n",
        "  df_compare['spread % '] = (df_compare['prediction'] - df_compare['actual'])/(df_compare['actual'])\n",
        "\n",
        "  print(df_compare)\n",
        "\n",
        "  df_compare.to_csv(\"/content/drive/MyDrive/Colab Notebooks/output data/\"+modelType+\"_\"+assetName+\"_results.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yK1CUH_4KIAf"
      },
      "outputs": [],
      "source": [
        "# ------------------------------------------------------------------------------------------START DEEP LEARNING EXECUTION FOR ALL ASSET CLASSES------------------------------------------------------------------------\n",
        "from sklearn.linear_model import LinearRegression, Lasso\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.metrics import plot_confusion_matrix, auc, roc_curve, plot_roc_curve\n",
        "\n",
        "lookback = 60\n",
        "hu = 30\n",
        "patience = 20\n",
        "epochs = 500\n",
        "batchsize = 64 # multiples of 8\n",
        "\n",
        "# features - convert to numpty\n",
        "SPY_feature_X = final_SPY_rawData.to_numpy()\n",
        "DIA_feature_X = final_DIA_rawData.to_numpy()\n",
        "QQQ_feature_X = final_QQQ_rawData.to_numpy()\n",
        "\n",
        "# these are Y values  - target is to train model to guess price. \n",
        "SPY_adjClose_Y = fe_df_SPY['Adj Close'].to_numpy()\n",
        "SPY_adjClose_Y = np.reshape(SPY_adjClose_Y, (-1,1))\n",
        "DIA_adjClose_Y = fe_df_DIA['Adj Close'].to_numpy()\n",
        "DIA_adjClose_Y = np.reshape(DIA_adjClose_Y, (-1,1))\n",
        "QQQ_adjClose_Y = fe_df_QQQ['Adj Close'].to_numpy()\n",
        "QQQ_adjClose_Y = np.reshape(QQQ_adjClose_Y, (-1,1))\n",
        "\n",
        "# executing single LSTM model for all three asset type - SPY, DIA, QQQ using StandardScaler or Gaussian Scaling\n",
        "#scaleAndTrainTestSplit(data_X, data_Y, scaler, assetName) \n",
        "#callModel(scaled_X_train, scaled_Y_train, scaled_X_test, scaled_Y_test, scaler_y, modelType, hu, lookback, ipShape_features, patience, epochs, batchsize, assetName)\n",
        "\n",
        "ipShape_features = final_SPY_rawData.shape[1]\n",
        "print(ipShape_features)\n",
        "scaled_feature_X_train, scaled_feature_X_test, scaled_feature_Y_train, scaled_feature_Y_test, scaler_x, scaler_y = scaleAndTrainTestSplit(SPY_feature_X, SPY_adjClose_Y,\"StandardScaler\", \"SPY\")\n",
        "callModel(scaled_feature_X_train, scaled_feature_Y_train, scaled_feature_X_test, scaled_feature_Y_test, scaler_y, 'singleLSTM', hu, lookback, ipShape_features, patience, epochs, batchsize, \"SPY\")\n",
        "\n",
        "ipShape_features = final_DIA_rawData.shape[1]\n",
        "scaled_feature_X_train, scaled_feature_X_test, scaled_feature_Y_train, scaled_feature_Y_test, scaler_x, scaler_y = scaleAndTrainTestSplit(DIA_feature_X, DIA_adjClose_Y,\"StandardScaler\", \"DIA\")\n",
        "callModel(scaled_feature_X_train, scaled_feature_Y_train, scaled_feature_X_test, scaled_feature_Y_test, scaler_y, 'singleLSTM', hu, lookback, ipShape_features, patience, epochs, batchsize, \"DIA\")\n",
        "\n",
        "ipShape_features = final_QQQ_rawData.shape[1]\n",
        "scaled_feature_X_train, scaled_feature_X_test, scaled_feature_Y_train, scaled_feature_Y_test, scaler_x, scaler_y = scaleAndTrainTestSplit(QQQ_feature_X, QQQ_adjClose_Y,'StandardScaler', \"QQQ\")\n",
        "callModel(scaled_feature_X_train, scaled_feature_Y_train, scaled_feature_X_test, scaled_feature_Y_test, scaler_y, 'singleLSTM', hu, lookback, ipShape_features, patience, epochs, batchsize, \"QQQ\")\n",
        "\n",
        "# executing stacked LSTM model for all three asset type - SPY, DIA, QQQ using StandardScaler or Gaussian Scaling\n",
        "\n",
        "ipShape_features = final_SPY_rawData.shape[1]\n",
        "scaled_feature_X_train, scaled_feature_X_test, scaled_feature_Y_train, scaled_feature_Y_test, scaler_x, scaler_y = scaleAndTrainTestSplit(SPY_feature_X, SPY_adjClose_Y,'StandardScaler', \"SPY\")\n",
        "callModel(scaled_feature_X_train, scaled_feature_Y_train, scaled_feature_X_test, scaled_feature_Y_test, scaler_y, 'stackedLSTM', hu, lookback,ipShape_features, patience, epochs, batchsize, \"SPY\")\n",
        "\n",
        "ipShape_features = final_DIA_rawData.shape[1]\n",
        "scaled_feature_X_train, scaled_feature_X_test, scaled_feature_Y_train, scaled_feature_Y_test, scaler_x, scaler_y = scaleAndTrainTestSplit(DIA_feature_X, DIA_adjClose_Y,'StandardScaler', \"DIA\")\n",
        "callModel(scaled_feature_X_train, scaled_feature_Y_train, scaled_feature_X_test, scaled_feature_Y_test, scaler_y, 'stackedLSTM', hu, lookback,ipShape_features, patience, epochs, batchsize,  \"DIA\")\n",
        "\n",
        "ipShape_features = final_QQQ_rawData.shape[1]\n",
        "scaled_feature_X_train, scaled_feature_X_test, scaled_feature_Y_train, scaled_feature_Y_test, scaler_x, scaler_y = scaleAndTrainTestSplit(QQQ_feature_X, QQQ_adjClose_Y,'StandardScaler', \"QQQ\")\n",
        "callModel(scaled_feature_X_train, scaled_feature_Y_train, scaled_feature_X_test, scaled_feature_Y_test, scaler_y, 'stackedLSTM', hu, lookback,ipShape_features, patience, epochs, batchsize, \"QQQ\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------------------------------------------------------SINGLE LSTM HYPER TUNE FUNCTION------------------------------------------------------------------------\n",
        "def hypertune_create_model_singleLSTM(hp):\n",
        "    print(\"hypertuning single LSTM model\")\n",
        "    \n",
        "    tensorflow.keras.backend.clear_session()   \n",
        "    \n",
        "    # Tune the number of units in the layers from 2 to 32 to obtain best value of hu\n",
        "    hu_units = hp.Int('units1', min_value=2, max_value=32, step=4)\n",
        "\n",
        "    # Tune the activation method to use\n",
        "    hp_activation = hp.Choice(name = 'activation', values = ['relu', 'elu'], ordered = False)    \n",
        "\n",
        "    # instantiate the model\n",
        "    model = Sequential()\n",
        "    #we will choose input shape of 60 by 100 because we are sticking 60 day lookback period and about 100 features for stock price prediction\n",
        "    model.add(LSTM(units=hu_units, input_shape=(60, 103), activation = hp_activation, return_sequences=False, name='LSTM'))\n",
        "    model.add(Dense(units=1, name='Output'))              # can also specify linear activation function \n",
        "    \n",
        "    # specify optimizer separately (preferred method))\n",
        "    #opt = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n",
        "    opt = Adam(lr=0.001, epsilon=1e-08, decay=0.0)       # adam optimizer seems to perform better for a single lstm\n",
        "    \n",
        "    # model compilation\n",
        "    model.compile(optimizer=opt, loss='mse', metrics=['mae', 'accuracy'])\n",
        "    \n",
        "    return model"
      ],
      "metadata": {
        "id": "RJ_ty3qEHxVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------------------------------------------------------STACKED LSTM HYPER TUNE FUNCTION------------------------------------------------------------------------\n",
        "def hypertune_create_model_stackedLSTM(hp):\n",
        "    print(\"hypertuning STACKED LSTM model\")\n",
        "    tensorflow.keras.backend.clear_session()   \n",
        "    \n",
        "    # Tune the number of units in the layers from 2 to 32 to obtain best value of hu\n",
        "    hu_units1 = hp.Int('units1', min_value=2, max_value=32, step=4)\n",
        "    hu_units2 = hp.Int('units1', min_value=2, max_value=32, step=4)\n",
        "    hu_units3 = hp.Int('units1', min_value=2, max_value=32, step=4)\n",
        "    # Tune the activation method to use\n",
        "    hp_activation1 = hp.Choice(name = 'activation', values = ['relu', 'elu'], ordered = False)    \n",
        "    hp_activation2 = hp.Choice(name = 'activation', values = ['relu', 'elu'], ordered = False)    \n",
        "    hp_activation3 = hp.Choice(name = 'activation', values = ['relu', 'elu'], ordered = False)    \n",
        "    #Tune dropout rates\n",
        "    hp_dropout1 = hp.Float('Dropout_rate', min_value=0, max_value=0.5, step=0.1)\n",
        "    hp_dropout2 = hp.Float('Dropout_rate', min_value=0, max_value=0.5, step=0.1)\n",
        "\n",
        "\n",
        "    # instantiate the model\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(units=hu_units1, input_shape=(60, 103), activation = hp_activation1, return_sequences=True, name='LSTM1'))\n",
        "    model.add(Dropout(hp_dropout1, name='Drouput1')) # 25% dropoutrate\n",
        "    model.add(LSTM(units=hu_units2, activation = hp_activation2, return_sequences=True, name='LSTM2'))\n",
        "    model.add(Dropout(hp_dropout2, name='Drouput2'))\n",
        "    model.add(LSTM(units=hu_units3, activation = hp_activation3, return_sequences=False, name='LSTM3'))    \n",
        "    model.add(Dense(units=1, name='Output'))              # can also specify linear activation function \n",
        "    \n",
        "    # specify optimizer separately (preferred method))\n",
        "    #opt = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n",
        "    opt = Adam(lr=0.001, epsilon=1e-08, decay=0.0)       # adam optimizer seems to perform better for a single lstm\n",
        "    \n",
        "    # model compilation\n",
        "    model.compile(optimizer=opt, loss='mse', metrics=['mae', 'accuracy'])\n",
        "    \n",
        "    return model"
      ],
      "metadata": {
        "id": "xQ15ZJYJxKKq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_L9hOLnTUQ4"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras-tuner"
      ],
      "metadata": {
        "id": "TqcaFXdUhWAb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------------------------------------------------------------------------------------START HYPER TUNING------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "_l7PdSu6uquI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1lHhNXlCTU7U"
      },
      "outputs": [],
      "source": [
        "# HYPER TUNE SINGLE LSTM\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import keras_tuner\n",
        "\n",
        "hp = keras_tuner.HyperParameters()\n",
        "\n",
        "# initialize an early stopping callback to prevent the model from\n",
        "# overfitting/spending too much time training with minimal gains\n",
        "callback1 = [EarlyStopping(patience=20, monitor='loss', mode='min', verbose=1, restore_best_weights=True),\n",
        "             TensorBoard(log_dir=\"./tensorboard/rslogs\")]\n",
        "\n",
        "%tensorboard --logdir ./tensorboard/rslogs\n",
        "\n",
        "scaled_feature_X_train, scaled_feature_X_test, scaled_feature_Y_train, scaled_feature_Y_test, scaler_x, scaler_y = scaleAndTrainTestSplit(DIA_feature_X, DIA_adjClose_Y,\"StandardScaler\", \"SPY\")\n",
        "X_train, Y_train = generate_sequence(scaled_feature_X_train, scaled_feature_Y_train, sequence_length=lookback)\n",
        "X_test, Y_test = generate_sequence(scaled_feature_X_test, scaled_feature_Y_train, sequence_length=lookback)\n",
        "\n",
        "# RandomSearch algorithm from keras tuner\n",
        "rstuner = keras_tuner.RandomSearch(\n",
        "    hypertune_create_model_singleLSTM,\n",
        "    objective=\"val_accuracy\",\n",
        "    max_trials=5,\n",
        "    directory=\"./keras\",\n",
        "    project_name=\"rstrail\",\n",
        "    overwrite=True\n",
        ")\n",
        "# launch tuning process\n",
        "rstuner.search(X_train, Y_train, epochs=50, validation_data=(X_test, Y_test), callbacks=callback1, shuffle=False )\n",
        "#rstuner.search(g, epochs=50, validation_data=g_, callbacks=callback1, class_weight = class_weight, shuffle=False)\n",
        "# display the best hyperparameter values for the model based on the defined objective function\n",
        "best_rshp = rstuner.get_best_hyperparameters()[0]\n",
        "print(best_rshp.values)\n",
        "# display tuning results summary\n",
        "rstuner.results_summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# HYPER TUNE STACKED LSTM\n",
        "\n",
        "# HYPER TUNE SINGLE LSTM\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import keras_tuner\n",
        "\n",
        "hp = keras_tuner.HyperParameters()\n",
        "\n",
        "# initialize an early stopping callback to prevent the model from\n",
        "# overfitting/spending too much time training with minimal gains\n",
        "callback1 = [EarlyStopping(patience=20, monitor='loss', mode='min', verbose=1, restore_best_weights=True),\n",
        "             TensorBoard(log_dir=\"./tensorboard/rslogs\")]\n",
        "\n",
        "%tensorboard --logdir ./tensorboard/rslogs\n",
        "\n",
        "scaled_feature_X_train, scaled_feature_X_test, scaled_feature_Y_train, scaled_feature_Y_test, scaler_x, scaler_y = scaleAndTrainTestSplit(DIA_feature_X, DIA_adjClose_Y,\"StandardScaler\", \"DIA\")\n",
        "X_train, Y_train = generate_sequence(scaled_feature_X_train, scaled_feature_Y_train, sequence_length=lookback)\n",
        "X_test, Y_test = generate_sequence(scaled_feature_X_test, scaled_feature_Y_train, sequence_length=lookback)\n",
        "\n",
        "# RandomSearch algorithm from keras tuner\n",
        "rstuner = keras_tuner.RandomSearch(\n",
        "    hypertune_create_model_stackedLSTM,\n",
        "    objective=\"val_accuracy\",\n",
        "    max_trials=5,\n",
        "    directory=\"./keras\",\n",
        "    project_name=\"rstrail\",\n",
        "    overwrite=True\n",
        ")\n",
        "# launch tuning process\n",
        "rstuner.search(X_train, Y_train, epochs=50, validation_data=(X_test, Y_test), callbacks=callback1, shuffle=False )\n",
        "#rstuner.search(g, epochs=50, validation_data=g_, callbacks=callback1, class_weight = class_weight, shuffle=False)\n",
        "# display the best hyperparameter values for the model based on the defined objective function\n",
        "best_rshp = rstuner.get_best_hyperparameters()[0]\n",
        "print(best_rshp.values)\n",
        "# display tuning results summary\n",
        "rstuner.results_summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lL4LQyO2zVq9",
        "outputId": "e4130a3d-31b1-4970-affe-3d3397d9ac5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 4 Complete [00h 35m 00s]\n",
            "val_accuracy: 0.0\n",
            "\n",
            "Best val_accuracy So Far: 0.0\n",
            "Total elapsed time: 02h 02m 05s\n",
            "\n",
            "Search: Running Trial #5\n",
            "\n",
            "Value             |Best Value So Far |Hyperparameter\n",
            "18                |22                |units1\n",
            "relu              |relu              |activation\n",
            "0.2               |0.5               |Dropout_rate\n",
            "\n",
            "hypertuning STACKED LSTM model\n",
            "WARNING:tensorflow:Layer LSTM1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer LSTM2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer LSTM3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Epoch 1/50\n",
            "141/141 [==============================] - 44s 293ms/step - loss: 26.8527 - mae: 1.0074 - accuracy: 0.0000e+00 - val_loss: 567.3817 - val_mae: 9.8701 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/50\n",
            "141/141 [==============================] - 43s 308ms/step - loss: 0.4604 - mae: 0.4820 - accuracy: 0.0000e+00 - val_loss: 30594.5762 - val_mae: 56.4444 - val_accuracy: 0.0000e+00\n",
            "Epoch 3/50\n",
            "141/141 [==============================] - 41s 287ms/step - loss: 0.2816 - mae: 0.4521 - accuracy: 0.0000e+00 - val_loss: 9875.3516 - val_mae: 23.4557 - val_accuracy: 0.0000e+00\n",
            "Epoch 4/50\n",
            "141/141 [==============================] - 40s 286ms/step - loss: 0.1438 - mae: 0.3214 - accuracy: 0.0000e+00 - val_loss: 89781.8047 - val_mae: 163.6450 - val_accuracy: 0.0000e+00\n",
            "Epoch 5/50\n",
            "141/141 [==============================] - 40s 284ms/step - loss: 0.2157 - mae: 0.3516 - accuracy: 0.0000e+00 - val_loss: 18601.4062 - val_mae: 53.5319 - val_accuracy: 0.0000e+00\n",
            "Epoch 6/50\n",
            "141/141 [==============================] - 42s 296ms/step - loss: 0.0820 - mae: 0.2314 - accuracy: 0.0000e+00 - val_loss: 7508.6045 - val_mae: 29.6389 - val_accuracy: 0.0000e+00\n",
            "Epoch 7/50\n",
            "141/141 [==============================] - 44s 310ms/step - loss: 0.0848 - mae: 0.2335 - accuracy: 0.0000e+00 - val_loss: 2799.6638 - val_mae: 11.7822 - val_accuracy: 0.0000e+00\n",
            "Epoch 8/50\n",
            "141/141 [==============================] - 40s 285ms/step - loss: 0.0609 - mae: 0.1969 - accuracy: 0.0000e+00 - val_loss: 4592.4766 - val_mae: 18.8239 - val_accuracy: 0.0000e+00\n",
            "Epoch 9/50\n",
            "141/141 [==============================] - 40s 286ms/step - loss: 0.1241 - mae: 0.2637 - accuracy: 0.0000e+00 - val_loss: 91.9793 - val_mae: 5.2696 - val_accuracy: 0.0000e+00\n",
            "Epoch 10/50\n",
            "141/141 [==============================] - 40s 281ms/step - loss: 0.0711 - mae: 0.2060 - accuracy: 0.0000e+00 - val_loss: 225.8932 - val_mae: 6.8214 - val_accuracy: 0.0000e+00\n",
            "Epoch 11/50\n",
            "141/141 [==============================] - 40s 284ms/step - loss: 0.0638 - mae: 0.1931 - accuracy: 0.0000e+00 - val_loss: 3515.5442 - val_mae: 21.5748 - val_accuracy: 0.0000e+00\n",
            "Epoch 12/50\n",
            "141/141 [==============================] - 40s 281ms/step - loss: 0.0803 - mae: 0.2094 - accuracy: 0.0000e+00 - val_loss: 395.7177 - val_mae: 7.6479 - val_accuracy: 0.0000e+00\n",
            "Epoch 13/50\n",
            "141/141 [==============================] - 44s 311ms/step - loss: 0.0626 - mae: 0.1833 - accuracy: 0.0000e+00 - val_loss: 1249.5271 - val_mae: 13.5607 - val_accuracy: 0.0000e+00\n",
            "Epoch 14/50\n",
            "141/141 [==============================] - 41s 293ms/step - loss: 0.0548 - mae: 0.1712 - accuracy: 0.0000e+00 - val_loss: 2441.0286 - val_mae: 21.7654 - val_accuracy: 0.0000e+00\n",
            "Epoch 15/50\n",
            "141/141 [==============================] - 40s 283ms/step - loss: 0.0626 - mae: 0.1736 - accuracy: 0.0000e+00 - val_loss: 135.5833 - val_mae: 5.8119 - val_accuracy: 0.0000e+00\n",
            "Epoch 16/50\n",
            "130/141 [==========================>...] - ETA: 3s - loss: 0.0405 - mae: 0.1421 - accuracy: 0.0000e+00"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "DL TimeSeries Aug20.ipynb",
      "provenance": [],
      "mount_file_id": "17EuGjM_4sp8gy9OG13HiMqBxD8Wc24VX",
      "authorship_tag": "ABX9TyNuWToucIK2OIXTl4Njc/UJ"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}